
## 사용 도구 및 기법

#### 1. 1gram ~ 5gram 까지의 n-gram을 사용(N-gram embedding)
* n-gram의 양극성(매파, 중립, 비둘기파)을 결정하기 위해 대조적이지만 상호 보완적인 두 가지 방법을 기반으로 두 가지 종류의 심리지표 개발(시장 접근법, 어휘 접근법 : market and lexical approaches)
* 시장 접근법 : 시장 정보(예: 주식 수익률)와의 통계적 연관성에 따라 단어의 극성 결정
* 어휘 접근법 : 미리 결정된 시드 단어와의 근접성을 기반으로 극성 결정, SentProp 최신 감정 유도 알고리즘 사용

<details>

**SentProp**는 자연어 처리(NLP) 분야에서 **도메인별 감정 사전**을 생성하기 위한 프레임워크입니다. 일반적인 감정 사전이 모든 맥락에서 단어의 감정 값을 제공하는 반면, SentProp은 특정 도메인이나 커뮤니티 내에서 단어의 감정을 반영하는 사전을 생성하는 데 중점을 둡니다. 이는 단어의 감정이 사용되는 맥락에 따라 크게 달라질 수 있기 때문에 중요합니다.

### SentProp의 주요 특징:

1. **도메인별 감정 유도**: SentProp은 특정 도메인의 텍스트 코퍼스에서 감정 사전을 도출하도록 설계되었습니다. 예를 들어, 'soft'라는 단어는 일반적인 맥락에서는 긍정적인 의미(예: "부드러운 베개")를 가질 수 있지만, 스포츠 맥락에서는 부정적인 의미(예: "부드러운 선수")를 가질 수 있습니다. SentProp은 이러한 뉘앙스를 도메인별 데이터를 통해 반영합니다.

2. **단어 임베딩 및 그래프 기반 기법 사용**: 이 프레임워크는 단어 임베딩(단어들 간의 의미적 관계를 포착하는 기법)과 그래프 기반 전파 알고리즘을 사용합니다. 단어들은 노드로, 이들 간의 관계(감정적 유사성)는 엣지로 표현되며, 소수의 알려진 감정 단어 세트를 통해 다른 단어들의 감정을 유추합니다.

3. **자원 효율성**: SentProp은 대규모 라벨링된 데이터 없이도 효과적으로 작동하도록 설계되었습니다. 이는 라벨링된 감정 데이터가 부족한 도메인에서 특히 유용합니다.

4. **역사적 및 커뮤니티별 응용**: SentProp은 단어 감정이 시간에 따라 어떻게 변화하는지(예: 19세기에서 21세기까지) 또는 다양한 온라인 커뮤니티 간에 어떻게 다르게 나타나는지 분석하는 데 사용되었습니다.

### 파이썬 예시

SentProp은 복잡한 알고리즘과 데이터 처리 과정을 포함하므로, 파이썬에서 이와 유사한 간단한 작업을 시뮬레이션하기 위해 다음과 같은 예시를 들어볼 수 있습니다. 이 예시는 단어 간의 유사성을 기반으로 간단한 그래프 전파를 사용하여 감정을 유추하는 과정을 보여줍니다.

```python
import networkx as nx

# 그래프 생성
G = nx.Graph()

# 노드 추가 (단어와 초기 감정 점수)
G.add_node('happy', sentiment=1.0)  # 긍정적
G.add_node('sad', sentiment=-1.0)   # 부정적
G.add_node('joyful')
G.add_node('depressed')

# 엣지 추가 (단어 간의 유사성)
G.add_edge('happy', 'joyful', weight=0.9)
G.add_edge('sad', 'depressed', weight=0.9)
G.add_edge('happy', 'depressed', weight=0.1)
G.add_edge('sad', 'joyful', weight=0.1)

# 감정 전파 (기본적 예시)
def propagate_sentiment(G):
    for node in G.nodes:
        if 'sentiment' not in G.nodes[node]:
            sentiment = 0
            total_weight = 0
            for neighbor in G.neighbors(node):
                sentiment += G[neighbor][node]['weight'] * G.nodes[neighbor]['sentiment']
                total_weight += G[neighbor][node]['weight']
            G.nodes[node]['sentiment'] = sentiment / total_weight if total_weight > 0 else 0
    return G

# 감정 전파 수행
G = propagate_sentiment(G)

# 결과 출력
for node in G.nodes(data=True):
    print(node)
```

이 코드에서는 단어들을 노드로, 단어 간의 유사성을 엣지로 표현한 그래프를 생성합니다. 초기에는 일부 단어들에만 감정 점수가 주어지고, 나머지 단어들은 그래프 전파를 통해 감정 점수를 유추하게 됩니다.

실제 SentProp은 이보다 훨씬 더 복잡한 알고리즘과 더 큰 데이터를 사용하여 동작하지만, 위 예시는 기본적인 아이디어를 이해하는 데 도움이 될 수 있습니다.


</details>

<br>

#### 2.  경제 분석을 위한 한국어 NLP 파이썬 라이브러리 eKoNLPy 활용
* eKoNLPy : https://github.com/entelecheia/eKoNLPy
* 한국어 '일드커브'(yield curve), '스티프닝'(steepening) 등을 인식하게 함

<details>

**eKoNLPy**는 한국어 자연어 처리(NLP)를 위한 파이썬 기반의 오픈소스 라이브러리입니다. 이 라이브러리는 한국어 형태소 분석, 명사 추출, 단어 빈도 분석 등을 수행할 수 있도록 설계되었습니다. eKoNLPy는 특히 다음과 같은 기능들을 제공합니다:

1. **형태소 분석**: 한국어 텍스트를 형태소 단위로 분리하고 각 형태소에 해당하는 품사를 태깅합니다. 이를 통해 문장 내에서 각 단어가 어떤 역할을 하는지 분석할 수 있습니다.

2. **명사 추출**: 텍스트에서 명사를 추출하는 기능을 제공합니다. 이는 키워드 추출이나 텍스트 요약 등의 작업에 유용하게 사용됩니다.

3. **단어 빈도 분석**: 주어진 텍스트 내에서 특정 단어가 얼마나 자주 등장하는지 분석할 수 있습니다. 이를 통해 텍스트의 주요 주제를 파악하거나, 중요한 키워드를 도출할 수 있습니다.

4. **다양한 사전 지원**: eKoNLPy는 여러 종류의 한국어 사전을 지원하여, 사용자가 분석하고자 하는 텍스트의 특성에 맞는 사전을 선택할 수 있도록 합니다.

### 주요 특징 및 장점:
- **한국어에 특화**: eKoNLPy는 한국어 처리에 특화되어 있어, 한국어 텍스트를 효과적으로 분석할 수 있습니다.
- **사용하기 쉬운 인터페이스**: 파이썬 기반으로, 사용자가 쉽게 접근하여 사용할 수 있는 API를 제공합니다.
- **오픈소스**: 누구나 자유롭게 사용하고 수정할 수 있으며, 지속적으로 업데이트되고 있습니다.

### 간단한 파이썬 예시:

```python
from eKoNLPy import Mecab

# Mecab 형태소 분석기 초기화
mecab = Mecab()

# 텍스트 예시
text = "eKoNLPy는 한국어 처리를 위한 도구입니다."

# 형태소 분석
morphs = mecab.morphs(text)
print("형태소 분석:", morphs)

# 명사 추출
nouns = mecab.nouns(text)
print("명사 추출:", nouns)

# 품사 태그 추가
pos = mecab.pos(text)
print("품사 태깅:", pos)
```

이 코드에서는 `Mecab` 형태소 분석기를 사용하여 텍스트를 분석하고, 형태소, 명사, 그리고 품사 태깅 결과를 출력합니다.

**eKoNLPy**는 자연어 처리 관련 연구나 실무 프로젝트에서 한국어 텍스트를 다루는 데 매우 유용한 도구입니다. 한국어 특유의 언어적 특징을 잘 반영한 라이브러리로, 다양한 언어 처리 작업에 활용될 수 있습니다.

</details>

<br>

#### 3. 감성 분석
언어에서 의견이나 태도 와 같은 주관적 정보를 감지하고 추출하기 위한 기법
* 관심 말뭉치 준비
* 텍스트 전처리
* 특징 선택
* 특징의 극성 또는 감성 분류
* 문장 및 문서의 감성 측정 

<br>

## 감성 분석(sentiment analysis)
* 이 논문에서는 2005년 5월 부터 2017년 12월 까지 231,699건의 문서가 수집되었습니다.

### 감성 분석의 절차

#### 1. 말뭉치 준비하기
* [151건의 금융통화위원회(Monetary Policy Board, MPB) 의사록](https://www.bok.or.kr/portal/singl/newsData/list.do?pageIndex=&targetDepth=3&menuNo=201154&syncMenuChekKey=1&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200038&depth3=201154&date=&sdate=&edate=&sort=1&pageUnit=10), 내용 중 두 번째와 세 번째 섹션만 사용
* 206,223건의 뉴스 기사, [네이버](https://news.naver.com)와 [인포맥스](http://news.einfomax.co.kr)에서 '금리' 라는 단어가 포함된 뉴스기사를 수집
* 25,325건의 [채권 애널리스트 보고서](https://www.wisereport.co.kr)

<br>

#### 2. 텍스트 사전 처리
**1. 토큰화와 정규화하기**
* 토큰화 : 긴 문자열의 텍스트를 작은 조각 또는 토큰(단어)으로 분할하는 단계
* 정규화 : 텍스트를 하나의 표준 형식으로 변환하는 프로세스,
	구두점 제거, 마침표 제거, 숫자를 해당 단어에 해당하는 단어로 변환, 어간, 철자법, 대소문자 접기 등이 포함

<br>

 **2. eKoNLPy 사용**
* 경제 용어 사전에서 확보한 4,202개의 분야별 용어를 사전 제공하여 한국어에 특화된 형태소 분석, 명사 추출, 단어 빈도 분석 등을 수행
* "금리 박스권 상단 상향과 일드 커브 완만한 스티프닝 전망" 등과 같은 경제 용어가 한국어로 쓰인 단어를 구분할 수 있음

<br>

#### 3. 특징 선택

**1. N-gram**
'회복'과 같은 단어는 단독적으로 긍정을 의미하지만 '회복 부진'이라는 문구는 그렇지 않은데, 이처럼 긍정과 부정 단어가 결합된 경우 감성을 측정하기 쉽지 않다. 이 문제를 해결하기 위해 N-gram을 이용한다.

* '차원의 저주' 문제 혹은 과적합을 방지하기 위해 N-gram을 5개로 설정
* 차원이 폭발적으로 증가하는걸 방지하기 위해 단어 태그를 명사(NNG), 형용사(VA, VAX), 부사(MAG), 동사(VA), 부정사로 제한
* 각 문장에서 겹치는 N-gram이 여러개 발견될 경우 가장 높은 N-gram만 고려
* 15회 미만 빈도의 N-gram은 삭제

이 논문에서는 2,712개의 단어 세트, 73,428개의 N-gram을 얻었음.

<br>

#### 4. 극성 분류
극성을 분류하는 방법은 두 가지가 있다.
* 시장 접근 방식 : 시장 정보로부터 극성을 분류
* 말뭉치 기반 접근법 : 단어(논문의 경우 N-gram)임베딩과 시드 단어를 사용하여 분류

*논문에서는 나이브 베이즈 분류기(NBC)를 사용했다.

**1. 시장 접근 방식**
* 뉴스 기사와 보고서가 발표된 날 콜금리의 1개월 변동이 양(+)이면 매파적, 반대의 경우 비둘기파적으로 분류하여 **말뭉치에 표시**
* 이렇게 라벨링된 문장을 훈련 세트와 테스트 세트로 9:1로 무작위로 나눈다.
* 각 문장에 대해 5gram(1~5gram)을 특징으로 사용해 분류기를 훈련하고 정확도를 확인
* 훈련된 NBC는 클래스(매파/비둘기파)에 따라 각 특징의 조건부 확률을 산출하며, 이를 특징의 극성 점수로 사용한다.

$$\text{polarityscore} = \frac{p(\text{feature} \mid \text{hawkish})}{p(\text{feature} \mid \text{dovish})} = \frac{\frac{p(\text{feature} \& \text{hawkish})}{p(\text{hawkish})}}{\frac{p(\text{feature} \& \text{dovish})}{p(\text{dovish})}}$$
* 좋은 예측 성능을 위해 이 절차를 30회 반복하고 극성의 점수의 평균을 최종 점수로 사용(bagging)

극성 점수가 1보다 크면 매파로 분류, 1보다 작으면 비둘기파로 분류한다. 논문의 경우 매파는 18,685개, 비둘기파는 21,280개
<br>

**2. 어휘 접근 방식** 
두 단어가 같은 문맥에서 자주 함께 사용되면 같은 극성을 갖는 경향이 있는데, 이를 이용해 다른 단어와의 상대적 발생 빈도를 계산해 미지의 단어의 극성을 결정할 수 있게함

*논문에서는 ngram2vec, SentProp 프레임워크를 이용

* 말뭉치 231,699개 문서 전체를 사용해 ngram2vec을 훈련
* N-gram벡터를 이용해 매파 및 비둘기파적 시드 세트의 무작위 집합 10개를 50회 반복해서 전파 실행(bootstrap), SentProp 사용
* 극성 점수가 1보다 크면 매파, 1보다 작으면 비둘기파 어휘로 분류

<br>

#### 5. 감성 측정
사전이 준비되면 마지막으로 대상 문서의 어조를 측정한다.

* 문장에 포함된 매파적 및 비둘기파적 특징의 수(N-gram)를 기준으로 문장의 어조를 계산

<br>

$$\text{tones} = \frac{\text{매파적 특징의 수} - \text{비둘기파적 특징의 수}}{\text{매파적 특징의 수} + \text{비둘기파적 특징의 수}}
$$



1. 경제지표
* GDP 성장률
* 인플레이션율(CPI)
* 실업률
2. 금융 시장 데이터
* 주식 시장 지수(예: KOSPI)
* 국채 수익률
* 환율
3. 심리지표
* 감성 점수(tones)
* 소비자 신뢰 지수
* 기업 신뢰 지수
4. 정책변수
* 중앙은행 기준금리
* 재정 지출 증가율
