## ë‰´ìŠ¤Â·ì±„ê¶ŒÂ·ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ ê°ì •ë¶„ì„ ê¸°ë°˜ ê¸ˆë¦¬ ë°©í–¥ì„± ì˜ˆì¸¡ 



<br>



## ğŸ”– ëª©ì°¨

- [í”„ë¡œì íŠ¸ ì†Œê°œ](#í”„ë¡œì íŠ¸-ì†Œê°œ)
- [ë©¤ë²„](#ë©¤ë²„)
- [Project Roadmap](#Project-Roadmap)



<br>



## í”„ë¡œì íŠ¸ ì†Œê°œ 

**Deciphering Moneetary Policy Board Minutes with Text Mining : The Case of South Korea** ë…¼ë¬¸ì„ êµ¬í˜„í•˜ëŠ” í”„ë¡œì íŠ¸ë¡œ, í•œêµ­ì€í–‰ ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡, ë‰´ìŠ¤ê¸°ì‚¬, ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ ìì—°ì–´ ì²˜ë¦¬, í† í”½ëª¨ë¸ë§, ê°ì„± ë¶„ì„ì„ í†µí•´ ë¶„ì„í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì— í•™ìŠµì‹œì¼œ ë‹¤ìŒ ê¸ˆë¦¬ì˜ ë°©í–¥ì„±ì„ ì˜ˆì¸¡í™¥ë‹ˆë‹¤.



<br>



## ë©¤ë²„

- [ê¹€ì€ì§€](https://github.com/eunji983dd)
- [ì¥ì¤€í˜](https://github.com/JangJune)
- [ìµœì¤€í˜](https://github.com/kimbap918)
- [í—ˆí™](https://github.com/0820hong)



<br>



## Project Roadmap

![](https://i.imgur.com/wh7YiFV.png)

1. ë¶„ì„Â·ê¸°íš

   * í”„ë¡œì íŠ¸ ëª©í‘œ ì •ì˜

   * í”„ë¡œì íŠ¸ ê³„íš ìˆ˜ë¦½

2. ë°ì´í„° ì¤€ë¹„
   * ë°ì´í„° ìˆ˜ì§‘ ë° í´ë Œì§•
   * ë°ì´í„° ì „ì²˜ë¦¬

3. ëª¨ë¸ë§
   * ëª¨ë¸ ê°œë°œ
   * ëª¨ë¸ ê²€ì¦ ë° í‰ê°€
   * ì‹œê°í™”

4. í‰ê°€

   * ê²°ë¡ 
   * ê¸°ëŒ€íš¨ê³¼
   * í•œê³„ì  ë° ê°œì„  ë°©ì•ˆ

   

<br>



## ë¶„ì„Â·ê¸°íš

### 1. í”„ë¡œì íŠ¸ ëª©í‘œ ì •ì˜

**"Deciphering Monetary Policy Board Minutes with Text Mining: The Case of South Korea"** ë…¼ë¬¸ êµ¬í˜„ í”„ë¡œì íŠ¸ë¡œ, ë‰´ìŠ¤ê¸°ì‚¬, ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸, í•œêµ­ì€í–‰ ê¸ˆìœµí†µí™” ìœ„ì›íšŒ ì˜ì‚¬ë¡ì„ í¬ë¡¤ë§í•˜ì—¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. 

ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ìì—°ì–´ ì²˜ë¦¬(NLP)ë¥¼ í†µí•´ ì •ì œí•œ í›„, Naive Bayes Classifierë¥¼ í™œìš©í•˜ì—¬ ê°ì •(ê¸ì •, ì¤‘ë¦½, ë¶€ì •)ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µê³„ ê¸°ë°˜ ëª¨ë¸ë§ì„ í†µí•´ ë‹¤ìŒ ê¸ˆë¦¬ì˜ ë°©í–¥ì„±ì„ ì˜ˆì¸¡í•˜ê³ ì í•©ë‹ˆë‹¤.



<br>



### 2. í”„ë¡œì íŠ¸ ê³„íš ìˆ˜ë¦½

1) **í•„ìš” ë°ì´í„° ì •ì˜**

   ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ : 2024-08-09 ~ 2024-08-19

   1) ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ : í•œêµ­ì€í–‰ì—ì„œ ì œê³µí•˜ëŠ” ê³µì‹ ë¬¸ì„œë¡œ, ì •ì±… ê²°ì •ê³¼ ê´€ë ¨ëœ ì£¼ìš” ë…¼ì˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
   2) ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸ : ê¸ˆìœµ ê¸°ê´€ ë° ì „ë¬¸ê°€ë“¤ì´ ì‘ì„±í•œ ë³´ê³ ì„œë¡œ, ì±„ê¶Œ ì‹œì¥ì˜ ë™í–¥ ë° ì „ë§ì„ ë‹¤ë£¹ë‹ˆë‹¤.
   3) ë‰´ìŠ¤ ê¸°ì‚¬ : 2005ë…„ ë¶€í„° 2024ë…„ê¹Œì§€ ê¸°ì¤€ê¸ˆë¦¬ ë°œí‘œì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
   4) ì½œ ê¸ˆë¦¬ : ë‹¨ê¸° ê¸ˆìœµ ì‹œì¥ì—ì„œì˜ ê¸ˆë¦¬ ë°ì´í„°ì…ë‹ˆë‹¤.
   5) ê¸°ì¤€ ê¸ˆë¦¬ : í•œêµ­ì€í–‰ì´ ì„¤ì •í•œ ê¸°ì¤€ê¸ˆë¦¬ ë°ì´í„°ì…ë‹ˆë‹¤.

   

2) **í¬ë¡¤ë§ ì—­í•  ë¶„ë‹´**

   1.  ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ : í¬ë¡¤ë§ ë° ë°ì´í„° ìˆ˜ì§‘(í—ˆí™)
   2. ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸ : ì£¼ìš” ì±„ê¶Œ ë¦¬í¬íŠ¸ í¬ë¡¤ë§(ì¥ì¤€í˜)
   3. ë‰´ìŠ¤ê¸°ì‚¬ :
      * í•œêµ­ê²½ì œ(104,438ê±´) : í¬ë¡¤ë§(ìµœì¤€í˜)
      * ë©”ì¼ê²½ì œ(114,739) : í¬ë¡¤ë§(í—ˆí™)
      * ì´ë°ì¼ë¦¬(177,096) : í¬ë¡¤ë§(ì¥ì¤€í˜)
   4. ì½œ ê¸ˆë¦¬ ë° ê¸°ì¤€ ê¸ˆë¦¬ : ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘(í—ˆí™)

   

3) **ë°ì´í„° ì „ì²˜ë¦¬**

   eKoNLPyì˜ MPKOí™œìš©

   * ë¬¸ì„œ N-gram í† í°í™”ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³ , ë¶„ì„œì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

   

4) **í†µê³„ ê¸°ë°˜ ëª¨ë¸ë§ **

   1. Naive Bayes Classification

      * ì‹œì¥ ì ‘ê·¼ ë°©ì‹ : ì½œ ê¸ˆë¦¬ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ê¸ˆë¦¬ ë³€ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

        1. N-gram í† í° ê·¹ì„± ì„ì˜ ê²°ì • : ì½œ ê¸ˆë¦¬ ë³€ë™ì„ ê¸°ì¤€ìœ¼ë¡œ í† í°ì˜ ê·¹ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)ì„ ë¼ë²¨ë§í•©ë‹ˆë‹¤.

        2. Naive Bayes í™•ë¥  ê³„ì‚° : ë¼ë²¨ë§ëœ ê·¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ Naive Bayes ëª¨ë¸ì„ í•™ìŠµ, ê° í† í°ì´ ë¬¸ì„œ ë‚´ì—ì„œ í•´ë‹¹ ê·¹ì„±ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        3. ë¬¸ì„œ ê·¹ì„± ì¹´ìš´íŒ… ë° ê¸ˆë¦¬ ì˜ˆì¸¡ : íŠ¹ì • ê¸°ê°„ ë™ì•ˆì˜ ë¬¸ì„œì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•œ ê·¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê¸ˆë¦¬ì˜ ìƒìŠ¹, í•˜ë½, ë™ê²° ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

           

5. **ëª¨ë¸ ê²€ì¦**

   ì‹¤ì œ ê¸°ì¤€ ê¸ˆë¦¬ì™€ ë¹„êµ

   * ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê¸ˆë¦¬ ë°©í–¥ì„±ê³¼ ì‹¤ì œ ë°œí‘œëœ ê¸°ì¤€ ê¸ˆë¦¬ë¥¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

     

6. **ì‹œê°í™”**

   * Word Cloud : ì£¼ìš” í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

   * Text Data Visualization : ë¬¸ì„œë³„ ê·¹ì„± ë° ê¸ˆë¦¬ ë³€ë™ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

     

7. **í‰ê°€**

   ì‹œì¥ ì ‘ê·¼ ë°©ì‹ í‰ê°€

   * í‰ê°€ ê¸°ì¤€ : ë§¤íŒŒ(ê¸ˆë¦¬ ì¸ìƒ í˜¹ì€ ë™ê²°)ì™€ ë¹„ë‘˜ê¸°íŒŒ(ê¸ˆë¦¬ ì¸í•˜) ê°ê°ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì§€í‘œë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
     1. Accuracy : ì˜ˆì¸¡ì˜ ì •í™•ë„
     2. Precision : ì˜ˆì¸¡í•œ ìƒìŠ¹ ë˜ëŠ” í•˜ë½ ì¤‘ ì‹¤ì œë¡œ í•´ë‹¹í•˜ëŠ” ë¹„ìœ¨
     3. Recall : ì‹¤ì œ ìƒìŠ¹ ë˜ëŠ” í•˜ë½ ì¤‘ ì˜ˆì¸¡ì— ì„±ê³µí•œ ë¹„ìœ¨
     4. F1 Score : Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· ì„ í†µí•œ í‰ê°€



<br>



## ë°ì´í„° ì¤€ë¹„

### 1. ë°ì´í„° ìˆ˜ì§‘ ë° í´ë Œì§•

1. ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ í¬ë¡¤ë§

   2005ë…„ë„ ì œ 12ì°¨ ì˜ì‚¬ë¡(2005.06.09) ë¶€í„° 2024ë…„ë„ ì œ 14ì°¨ ì˜ì‚¬ë¡(2024.07.18)ê¹Œì§€ ì´ 390ê°œì˜ PDFíŒŒì¼ì„ í¬ë¡¤ë§ í–ˆìŠµë‹ˆë‹¤.

   <details>
     <summary>Python Code</summary>


     ```python
     import requests
     from bs4 import BeautifulSoup
     from selenium import webdriver
     from selenium.webdriver.common.by import By
     from selenium.webdriver.support.ui import WebDriverWait
     from selenium.webdriver.support import expected_conditions as EC
     from selenium.common.exceptions import TimeoutException, NoSuchElementException
     import time
     import pandas as pd
     import pymupdf as pm
     # %pip install PyMuPDF
     # %pip install requests_html
   
     webpage_num = [i for i in range(1, 40)]
     url_page_list = [f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?pageIndex={i}&targetDepth=3&menuNo=201154&syncMenuChekKey=2&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200038&depth3=201154&depth4=200789" for i in webpage_num]
     url = url_page_list[0]
     res = requests.get(url)
     soup = BeautifulSoup(res.text, 'html.parser')
     hrefs = soup.select("div.bd-line > ul > li.bbsRowCls")
     href_list = [href.select('.title')[0].attrs.get('href') for href in hrefs]
     "https://www.bok.or.kr"+href_list[0]
   
     # ì˜ì‚¬ë¡ ë‚ ì§œ list
     minute_date_list = []
     for url_page in url_page_list:
         dateres = requests.get(url_page, 'html.parser')
         datesoup = BeautifulSoup(dateres.text)
         titlelist = datesoup.select('li.bbsRowCls > div.set')
         for title in titlelist:
             text_split = title.text.split('\n')
             text_strip = [i.strip() for i in text_split if i.strip()][0]
             minute_date = text_strip[text_strip.index(')') + 2:-1]
             minute_date_list.append(minute_date)
   
     minute_date_list
     len(minute_date_list)
   
     # pdf ë§í¬ê°€ ìˆëŠ” ì‚¬ì´íŠ¸ ì¶”ì¶œ(2024ë…„ 7ì›” 18ì¼ ~ 2005ë…„ 6ì›” 9ì¼)
     link_list = []
     webpage_num = [i for i in range(1,40)]
     url_page_list = [f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?pageIndex={i}&targetDepth=3&menuNo=201154&syncMenuChekKey=2&depthSubMain=&subMainAt=&searchCnd=1&searchKwd=&depth2=200038&depth3=201154&depth4=200789" for i in webpage_num]
     for url in url_page_list:
         res = requests.get(url)
         soup = BeautifulSoup(res.text, 'html.parser')
         hrefs = soup.select("div.bd-line > ul > li.bbsRowCls")
         href_list = [href.select('.title')[0].attrs.get('href') for href in hrefs]
         link_list.extend(["https://www.bok.or.kr"+sub_href for sub_href in href_list])
   
     link_list
   
     # pdf ë§í¬ ì¶”ì¶œ
     pdf_link_list = []
     for sub_link in link_list:
         res2 = requests.get(sub_link, 'html.parser')
         soup2 = BeautifulSoup(res2.text)
         sub_pdf_link = soup2.select_one(".viewer").attrs.get('href')
         pdf_link_list.append("https://www.bok.or.kr"+sub_pdf_link)
     len(pdf_link_list)
     pdf_test_link = pdf_link_list[0]
     res3 = requests.get(pdf_test_link, 'html.parser')
     soup3 = BeautifulSoup(res3.text)
     pdf_test_link
     import fitz
   
     pdf_file_list = []
     for link in link_list:
         res4 = requests.get(link, 'html.parser')
         soup4 = BeautifulSoup(res4.text)
         if soup4.select('.file')[0].attrs.get('href')[-3:] == 'hwp':
             pdf_file_link = soup4.select('.file')[1]
         else:
             pdf_file_link = soup4.select('.file')[0]
         pdf_file_list.append("https://www.bok.or.kr" + pdf_file_link.attrs.get('href'))
   
     # for i in range(len(pdf_file_list)):
     #     if pdf_file_list[i][-3:] == 'hwp':
     #         pdf_file_list[i] = pdf_file_list[i].replace('hwp', 'pdf')
             
     pdf_file_list
     len(pdf_file_list)
     pdf_dict = {}
     for minute_date, pdf_file in zip(minute_date_list, pdf_file_list):
         try:
             pdf_res = requests.get(pdf_file)
             pdf_data = fitz.io.BytesIO(pdf_res.content)
             doc = fitz.open(stream=pdf_data, filetype="pdf")
             full_text = ""
             for page_num in range(len(doc)):
                 page = doc.load_page(page_num)  # Load the page
                 text = page.get_text("text")    # Extract text as a string
                 full_text += text+"\n"
                 
             keyword = "íšŒì˜ê²½ê³¼"
             keyword_index = full_text.index(keyword)
             full_text[keyword_index:]
             pdf_dict[minute_date] = full_text[keyword_index:]
   
         except Exception as e:
             # print('>>>', e)
             print(pdf_file)
             
   
     pdf_dict.keys()
     minute_df = pd.DataFrame(pdf_dict.values(), index=pdf_dict.keys())
     minute_df.rename(columns={0:'content'}, inplace=True)
     minute_df = minute_df.iloc[::-1]
     idxlist = []
     for item in minute_df.index:
         idxlist.append(item.replace('.','-'))
   
     minute_df.index = idxlist
     minute_df.reset_index(inplace=True)
     idxlist
     x = idxlist[0].split('-')
     for i in range(len(x)):
         if int(x[i]) < 10:
             x[i] = '0'+x[i]
   
     ''.join(x)
     final_idxlist = []
     for idx in idxlist:
         temp = idx.split('-')
         temp = [v for v in temp if v]
         for i in range(len(temp)):
             if int(temp[i])<10:
                 temp[i] = '0'+temp[i]
   
         final_idxlist.append(''.join(temp))
   
     final_idxlist
     minute_df['index'] = final_idxlist
     minute_df.rename(columns={'index' : 'date'}, inplace=True)
     minute_df
     minute_df.to_csv("C:/Users/SesacPython/Desktop/workspace/Team2/hong/mpc_minute.csv")
     ```

   </details>

   

   

2. ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸ í¬ë¡¤ë§

   ì´ 6,334ê°œì˜ ë„¤ì´ë²„ ì±„ê¶Œë¶„ì„ ë¦¬í¬íŠ¸ PDF íŒŒì¼ì„ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§í•œ ë¦¬í¬íŠ¸ PDF íŒŒì¼ ë‚´ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì„œ ë‚ ì§œë³„ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.

   <details>
     <summary>Python Code</summary>


     ```python
   from bs4 import BeautifulSoup
   import requests
   import time
   import re
   from requests import get
   from urllib import request
   from PyPDF2 import PdfReader
   import os
   import csv
   from pykospacing import Spacing
   import os.path
   import shutil
   from tika import parser
   from datetime import datetime
   
   today = datetime.today().strftime("%Y-%m-%d")
   
   target_day = "2013-05-09"
   
   url = f"https://finance.naver.com/research/debenture_list.naver?keyword=&brokerCode=&searchType=writeDate&writeFromDate={target_day}&writeToDate={today}&x=0&y=0&page=1"
   temp_url = "https://finance.naver.com/"
   headers = {
       'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 
       'Referer': 'https://www.naver.com/'
       }
   
   response = requests.get(url, headers=headers)
   
   soup = BeautifulSoup(response.text, 'html.parser')
   
   last_page = soup.select_one('td.pgRR>a').attrs['href']
   temp = "/research/debenture_list.naver?keyword=&brokerCode=&searchType=writeDate&writeFromDate=2013-05-09&writeToDate=2023-09-01&x=0&y=0&page="
   pages = int(last_page.replace(temp,''))
   
   for page in range(1,pages+1):
       page_url = f"https://finance.naver.com/research/debenture_list.naver?keyword=&brokerCode=&searchType=writeDate&writeFromDate={target_day}&writeToDate={today}&x=0&y=0&page={page}"
   
       headers = {
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 
           'Referer': 'https://www.naver.com/'
           }
   
       response = requests.get(page_url, headers=headers)
   
       soup = BeautifulSoup(response.text, 'html.parser')
   
       stock_links = soup.select('table.type_1> tr')
   
       nomal_page = "https://finance.naver.com/research/"
       print(f"{page} ë²ˆ í˜ì´ì§€ ì§„í–‰ì¤‘ì…ë‹ˆë‹¤")
       for link in stock_links:
           taget = link.select("a")
           if taget == []:
               pass
           else:
               url_link = taget[0].attrs['href']
               crawling_link = nomal_page+url_link
               total_url = crawling_link
   
               headers = {
                   'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36', 
                   'Referer': 'https://www.naver.com/'
                   }
   
               response = requests.get(total_url, headers=headers)
               print(total_url)
               soup = BeautifulSoup(response.text, 'html.parser')
   
               maket = soup.select_one('p.source').text
               maket = maket.replace("\n",'')
               maket = maket.replace("\t",'')
               maket = maket.split("|")
               maket_name = maket[0]
               date = maket[1]
   
               stock_title = soup.select_one('th.view_sbj').text
   
               title = stock_title.replace("\n",'')
               title = title.replace("\t",'')
               title = title.split(maket_name)
               title = title[0]
               title = title.replace('  ','') #ë¦¬ì„œì¹˜ ì œëª©
               for idx in range(len(title)): #ê³µë°±ì œê±°
                   if title[idx] != ' ':
                       target_idx = idx
               title = title[:target_idx+1]
               new_title = ''
               for i in title:
                   if i == "/":
                       new_title += "_"
                   else:
                       new_title += i
   
               title = new_title
               print(f"ë¦¬ì„œì¹˜ ì œëª© : {title}")
               info = soup.select_one("td.view_cnt").text
               info = info.replace('\n','')
               pdf = soup.select("a.con_link")[1].text
               if pdf == '':
                   print("PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ ì´ ê²½ìš°ì—” ìˆ˜ì§‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                   pass
               else:
                   print("PDF íŒŒì¼ì´ ì¡´ì¬ í•©ë‹ˆë‹¤")
                   info = info.split(pdf)
                   info = info[0]
                   pdf_link = soup.select_one("a.con_link").attrs['href'] #pdf ë‹¤ìš´ë¡œë“œ ë§í¬
                   pdf = soup.select("a.con_link")[1].text #pdfíŒŒì¼ ì´ë¦„
                   if "/" in pdf:
                       pdf = pdf.replace('/','_')
                   pdf_name = soup.select("a.con_link")[1].text[:-4]
                   print(pdf)
                   with open(pdf, "wb") as file:   # open in binary mode
                       response = get(pdf_link)               # get request
                       time.sleep(1)
                       file.write(response.content) 
   
                   PDF_FILE_PATH = pdf
   
                   # reader = PdfReader(PDF_FILE_PATH)
                   # pages = reader.pages
                   # text = ''
                   # for page in pages:
                   #     sub = page.extract_text()
                   #     text += sub
                   parsed = parser.from_file(PDF_FILE_PATH)
                   text = parsed['content']
                   if text is None:
                       print("pdf íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                   else:
                       str_text = text
                       new_str = re.sub("\n", "", str_text)
                       new_str = re.sub(" ", "", new_str)
   
                       spacing = Spacing()
                       kospacing_result = spacing(new_str) 
   
                       directory = f'./dataset/{date}'
   
                       if os.path.isdir(directory):
                           print(directory)
                           print("ì €ì¥ê²½ë¡œ ìˆìŠµë‹ˆë‹¤")
                           time.sleep(1)
                       else:
                           print("ì €ì¥ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤")
                           current_path = os.getcwd()
                           os.mkdir(directory)
                           time.sleep(2)
                           if os.path.isdir(directory):
                               print("ì €ì¥ê²½ë¡œ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤")
                           else:
                               print("ì €ì¥ê²½ë¡œ ìƒì„± ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤")
                       with open(f'{directory}/{title}_{maket_name}_{date}.text', 'w', encoding='utf-8') as f:
                           f.write('ì œëª©\n')
                           f.write(f'{title}\n')
                           f.write('\n')
                           f.write('ìš”ì•½\n')
                           f.write(f'{info}\n')
                           f.write('\n')
                           f.write('ë‚´ìš©\n')
                           f.write(f'{kospacing_result}\n')
                       time.sleep(1)
                   # íŒŒì¼ ì‚­ì œ
                   remove_file_path = PDF_FILE_PATH # ì œê±°í•  íŒŒì¼
                   os.remove(remove_file_path)
     ```

   </details>

   

   

   

3. ë‰´ìŠ¤ í¬ë¡¤ë§ - ê²€ìƒ‰ì°½ì— â€˜ê¸ˆë¦¬â€™ ê²€ìƒ‰

   - **ì´ë°ì¼ë¦¬**

     - **í¬ë¡¤ë§ ê°œìš”**

       ì´ë°ì¼ë¦¬ì—ì„œ 2005ë…„ 05ì›” 13ì¼ë¶€í„° 2024ë…„ 07ì›” 10ì¼ê¹Œì§€ ì œëª©ê³¼ ë³¸ë¬¸ì„ í¬ë¡¤ë§ í–ˆìŠµë‹ˆë‹¤. 2005ë…„ 06ì›” 09ì¼ì´ ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ì˜ pdf íŒŒì¼ì„ ìµœì´ˆë¡œ í¬ë¡¤ë§ ê°€ëŠ¥í•œ ë‚ ì§œê¸° ë•Œë¬¸ì— ê¸°ì¤€ìœ¼ë¡œ ì •í–ˆìŠµë‹ˆë‹¤. ì´ 177,096ê±´ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.

     - **í¬ë¡¤ë§ ì½”ë“œ**

       ```python
       import scrapy
       import pandas as pd
       
       # CSV íŒŒì¼ ì½ê¸°
       df = pd.read_csv('edaily_date.csv')
       
       class EdailySpider(scrapy.Spider):
           name = 'edaily'
           allowed_domains = ["edaily.co.kr"]
       
           def start_requests(self):
               x = 0
               for index, row in df.iterrows():
                   start_date = row['start']
                   end_date = row['end']
                   announce_date = int(row['announce_date'])
                   url = f'<https://www.edaily.co.kr/search/news/?source=total&keyword=%ea%b8%88%eb%a6%ac&include=&exclude=&jname=&start={start_date}&end={end_date}&sort=&date=pick&exact=false&page=1>'
                   yield scrapy.Request(url, self.parse, meta={'start_date': start_date, 'end_date': end_date, 'page': 1, 'announce_date': announce_date})
       
           def parse(self, response):
               # ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê° ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë§í¬ë¥¼ ì¶”ì¶œ
               article_links = response.css('div.newsbox_04 a::attr(href)').getall()
               
               # ë§Œì•½ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ë‹¤ê³  íŒë‹¨
               if not article_links:
                   self.logger.info(f"No articles found on page {response.url}. No more pages.")
                   return
       
               for link in article_links:
                   yield response.follow(link, self.parse_article, meta={'announce_date': response.meta['announce_date']})
               
               # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
               current_page = response.meta['page']
               next_page = current_page + 1
               next_page_url = f'<https://www.edaily.co.kr/search/news/?source=total&keyword=%ea%b8%88%eb%a6%ac&include=&exclude=&jname=&start={response.meta["start_date"]}&end={response.meta["end_date"]}&sort=&date=pick&exact=false&page={next_page}>'
               
               # í˜ì´ì§€ì— ê¸°ì‚¬ ë§í¬ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
               if article_links:
                   yield scrapy.Request(next_page_url, self.parse, meta={'start_date': response.meta['start_date'], 'end_date': response.meta['end_date'], 'page': next_page, 'announce_date': response.meta['announce_date']})
       
           def parse_article(self, response):
               # ë‰´ìŠ¤ ë³¸ë¬¸ê³¼ ì œëª© ì¶”ì¶œ
               title = response.css('div.news_titles h1::text').get()
               content = response.css('div.news_body::text').getall()
               
               # ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
               if not title:
                   self.logger.error(f"Missing title for URL: {response.url}")
                   return
               
               title = title.strip()
               content = ''.join(content).strip()
               announce_date = response.meta['announce_date']
               
               if 'ì¬ì†¡' in title:
                   self.logger.info(f"Skipping duplicated article: {title}")
                   return
               
               yield {
                   'announce_date': announce_date,
                   'title': title,
                   'content': content,
                   'url': response.url
               }
       ```

   - **í•œêµ­ê²½ì œ**

     - **í¬ë¡¤ë§ ê°œìš”**

       í•œêµ­ê²½ì œì—ì„œ 2005ë…„ 06ì›” 09ì¼ ë¶€í„° 2024ë…„ 07ì›” 11ì¼ê¹Œì§€ ë‰´ìŠ¤ ê¸°ì‚¬ ê±´ì˜ ë³¸ë¬¸ì„ scrapyë¥¼ ì´ìš©í•˜ì—¬ 104,438ê±´ í¬ë¡¤ë§ í–ˆìŠµë‹ˆë‹¤. ê¸ˆìœµí†µí™”ìœ„ì›íšŒ ì˜ì‚¬ë¡ì˜ pdfí¬ë¡¤ë§ ê°€ëŠ¥í•œ ì¼ìì™€ ë§ì¶”ê¸° ìœ„í•´ 2005ë…„ 06ì›” 09ë…„ì„ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ ì¡ì•˜ìŠµë‹ˆë‹¤.

     - **í¬ë¡¤ë§ ì½”ë“œ**

       ```python
       import pandas as pd
       import subprocess
       import os
       
       # CSV íŒŒì¼ ì½ê¸°
       date_ranges = pd.read_csv('/content/date_range.csv', sep=',')
       
       # ì—´ ì´ë¦„ í™•ì¸ (ì²« ë²ˆì§¸ ëª‡ ì¤„ ì¶œë ¥)
       print(date_ranges.head())
       
       # ì‹¤ì œ í—¤ë” ì´ë¦„ì„ í™•ì¸í•˜ì—¬ 'start'ì™€ 'end'ê°€ ë§ëŠ”ì§€ í™•ì¸
       # ë§Œì•½ í—¤ë”ê°€ ë‹¤ë¥´ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •
       # date_ranges.columns = ['start', 'end']  # ì‹¤ì œ íŒŒì¼ì˜ í—¤ë”ì— ë§ì¶° ìˆ˜ì •
       
       # ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
       date_ranges['start'] = pd.to_datetime(date_ranges['start'], format='%Y.%m.%d')
       date_ranges['end'] = pd.to_datetime(date_ranges['end'], format='%Y.%m.%d')
       
       # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
       output_dir = 'output'
       os.makedirs(output_dir, exist_ok=True)
       
       # ê° ë‚ ì§œ ë²”ìœ„ë¥¼ ìˆœíšŒí•˜ë©° Scrapy ëª…ë ¹ ì‹¤í–‰
       for index, row in date_ranges.iterrows():
           start_date = row['start'].strftime('%Y.%m.%d')
           end_date = row['end'].strftime('%Y.%m.%d')
       
           # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¼ ì¶œë ¥ íŒŒì¼ ì´ë¦„ ì •ì˜
           output_file = os.path.join(output_dir, f'hankyung_news_{start_date}_to_{end_date}.json')
       
           # ì‹¤í–‰í•  Scrapy ëª…ë ¹ì–´
           command = [
               'scrapy', 'runspider', 'hankyung.py',
               '-a', f'search_term=ê¸ˆë¦¬',
               '-a', f'start_date={start_date}',
               '-a', f'end_date={end_date}',
               '-o', output_file,
               '--loglevel=DEBUG'  # ë¡œê·¸ ë ˆë²¨ì„ DEBUGë¡œ ì„¤ì •
           ]
       
           # ëª…ë ¹ì–´ ì‹¤í–‰ ë° ë¡œê·¸ ìº¡ì²˜
           result = subprocess.run(command, capture_output=True, text=True)
       
           # í‘œì¤€ ì¶œë ¥ ë° ì˜¤ë¥˜ ì¶œë ¥ ë¡œê·¸ í‘œì‹œ
           print(f"Output for {start_date} to {end_date}:")
           print(result.stdout)
           if result.stderr:
               print(f"Errors for {start_date} to {end_date}:")
               print(result.stderr)
       
           # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
           if result.returncode != 0:
               print(f"Scrapy command failed for {start_date} to {end_date}.")
       ```

   - **ë§¤ì¼ê²½ì œ**

     - **í¬ë¡¤ë§ ê°œìš”**

       ì—°í•©ì¸í¬ë§¥ìŠ¤ì—ì„œ 2013ë…„ 05ì›” 09ì¼ ë¶€í„° 2013ë…„ 09ì›” 01ì¼ê¹Œì§€ í¬ë¡¤ë§ í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•œ ì ì ˆí•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ 10ë…„ ì´ìƒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ë ¤ í–ˆìŠµë‹ˆë‹¤. 2013ë…„ 05ì›” 09ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡ì€ ì´ìœ ëŠ” ê¸°ì¤€ê¸ˆë¦¬ ë³€ê²½ì¼ì´ì—ˆê¸° ë•Œë¬¸ì— ê¸°ì¤€ì ìœ¼ë¡œ ì •í–ˆìŠµë‹ˆë‹¤. ì´ 114,739ê±´ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.

     - **í¬ë¡¤ë§ ì½”ë“œ**

       ```python
       import requests
       from bs4 import BeautifulSoup
       import os
       
       # í¬ë¡¤ë§í•  í˜ì´ì§€ì˜ ìˆ˜ë¥¼ ì¶”ì¶œ
       url = f"<https://news.einfomax.co.kr/news/articleList.html?page=1&total=6417&sc_section_code=&sc_sub_section_code=&sc_serial_code=&sc_area=A&sc_level=&sc_article_type=&sc_view_level=&sc_sdate=2019-01-01&sc_edate=2019-12-31&sc_serial_number=&sc_word=%EA%B8%88%EB%A6%AC&box_idxno=&sc_multi_code=&sc_is_image=&sc_is_movie=&sc_user_name=&sc_order_by=E&view_type=sm>"
       headers = {
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
           'Referer': '<https://www.naver.com/>'
           }
       base = '<https://news.einfomax.co.kr>'
       response = requests.get(url, headers=headers)
       soup = BeautifulSoup(response.text, 'html.parser')
       total = soup.select_one('#sections > section > header > h3 > small').text
       total_num = ''
       for i in total:
           if i.isdigit():
               total_num += i
       total_num = int(total_num)
       page_num_0 = total_num / 20
       page_num_1 = total_num // 20
       if page_num_0 != page_num_1:
           pages = page_num_1 + 1
       else:
           pages = page_num_1
       pages #1~í˜ì´ì§€ë¶€í„° pages ê¹Œì§€ í¬ë¡¤ë§
       for page in range(1,pages+1):
           url = f"<https://news.einfomax.co.kr/news/articleList.html?page={page}&total=6417&sc_section_code=&sc_sub_section_code=&sc_serial_code=&sc_area=A&sc_level=&sc_article_type=&sc_view_level=&sc_sdate=2019-01-01&sc_edate=2019-12-31&sc_serial_number=&sc_word=%EA%B8%88%EB%A6%AC&box_idxno=&sc_multi_code=&sc_is_image=&sc_is_movie=&sc_user_name=&sc_order_by=E&view_type=sm>"
           headers = {
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
               'Referer': '<https://www.naver.com/>'
               }
           base = '<https://news.einfomax.co.kr>'
           response = requests.get(url, headers=headers)
           soup = BeautifulSoup(response.text, 'html.parser')
           li_tg = soup.select('ul.type2>li>h4.titles>a') #í•´ë‹¹ í˜ì´ì§€ì— ìˆëŠ” ë‰´ìŠ¤ê¸°ì‚¬ ë§í¬ ë¦¬ìŠ¤íŠ¸
           for i in li_tg:
               target = i.attrs['href']
               crawling_url = base + target
               response = requests.get(crawling_url, headers=headers)
               crawling_soup = BeautifulSoup(response.text, 'html.parser') # í•´ë‹¹ ë‰´ìŠ¤ê¸°ì‚¬ ë§í¬ì˜ html ì •ë³´ ì¶”ì¶œ
               title = crawling_soup.select_one('h3.heading').text
               new_title = '' #íƒ€ì´í‹€ ì „ì²˜ë¦¬ ê²°ê³¼
               for i in title:
                   if i == "/":
                       new_title += "_"
                   else:
                       new_title += i
               date_li = crawling_soup.select('ul.infomation>li')[1].text
               date_li = date_li.split("ì…ë ¥")
               date = date_li[-1]
               date = date.replace('.','_')
               date = date.replace(':','_')
               
               date #ë‚ ì§œ
               info = crawling_soup.select_one('#article-view-content-div').text
               info = info.replace('\\n','')
               info = info.replace('\\r','')
               info = info.replace('\\t','')
               info #ë‚´ìš©
               
               #íŒŒì¼ ì‘ì„±
               print(f'./news/2019{date}_ì—°í•©ì¸í¬ë§¥ìŠ¤.text')
               with open(f'./news/2019/{date}_ì—°í•©ì¸í¬ë§¥ìŠ¤.text', 'w', encoding='utf-8') as f:
                   f.write('ì œëª©\\n')
                   f.write(f'{new_title}\\n')
                   f.write('\\n')
                   f.write('ë‚´ìš©\\n')
                   f.write(f'{info}\\n')
       ```

   
